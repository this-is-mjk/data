{
    "relevant_link": [
        {
            "url": "https://help.medium.com/hc/en-us?source=post_page-----d740493ff328--------------------------------",
            "title": "Medium Help Center"
        },
        {
            "url": "https://medium.com/@nicholas.michael.janulewicz/list/chatgpt-prompts-b4c47b8e12ee?source=read_next_recirc-----d740493ff328--------------------------------",
            "title": "List: ChatGPT prompts  | Curated by Nicholas Michael Janulewicz | Medium"
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3ddf1357f371&operation=register&redirect=https%3A%2F%2Fvijaykumarkartha.medium.com%2Fbeginners-guide-to-conversational-retrieval-chain-using-langchain-3ddf1357f371&source=-----d740493ff328----0-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Medium"
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0189a18ec401&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40suraj_bansal%2Fbuild-your-own-ai-chatbot-a-beginners-guide-to-rag-and-langchain-0189a18ec401&source=-----d740493ff328----2-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Medium"
        },
        {
            "url": "https://medium.com/@MediumStaff/list/staff-picks-c7bc6e1ee00f?source=read_next_recirc-----d740493ff328--------------------------------",
            "title": "List: Staff Picks | Curated by Medium Staff | Medium"
        },
        {
            "url": "https://medium.com/@suraj_bansal?source=read_next_recirc-----d740493ff328----2---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Suraj Bansal – Medium"
        },
        {
            "url": "https://medium.com/about?autoplay=1&source=post_page-----d740493ff328--------------------------------",
            "title": "About Medium"
        },
        {
            "url": "https://vijaykumarkartha.medium.com/?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Vijaykumar Kartha – Medium"
        },
        {
            "url": "https://medium.com/?source=---two_column_layout_nav----------------------------------",
            "title": "Medium: Read and write stories."
        },
        {
            "url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----d740493ff328--------------------------------",
            "title": "Careers at Medium | by Jobs @ Medium | Jobs at Medium | Medium"
        },
        {
            "url": "https://medium.com/@atulkumar_68871/incorporating-chat-history-in-conversational-agents-using-langchain-9045b85edb78?source=read_next_recirc-----d740493ff328----3---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Incorporating Chat History in Conversational Agents using LangChain | by Atul Kumar | Medium"
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2970140edf33&operation=register&redirect=https%3A%2F%2Flmy.medium.com%2Fcomparing-langchain-and-llamaindex-with-4-tasks-2970140edf33&source=-----d740493ff328----0-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3Dd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=-----d740493ff328---------------------post_audio_button-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F793fb9c3580c&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40talon8080%2Fmastering-rag-chabots-the-road-from-a-poc-to-production-793fb9c3580c&source=-----d740493ff328----1-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/tag/langchain?source=post_page-----d740493ff328---------------langchain-----------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd740493ff328&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=---post_footer_upsell--d740493ff328---------------------lo_non_moc_upsell-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc3c8261f66b3&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&newsletterV3=55ec5990cf02&newsletterV3Id=c3c8261f66b3&user=Tal+Waitzenberg&userId=55ec5990cf02&source=-----d740493ff328---------------------subscribe_user-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9045b85edb78&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40atulkumar_68871%2Fincorporating-chat-history-in-conversational-agents-using-langchain-9045b85edb78&source=-----d740493ff328----3-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.statuspage.io/?source=post_page-----d740493ff328--------------------------------",
            "title": "Medium Status"
        },
        {
            "url": "https://lmy.medium.com/comparing-langchain-and-llamaindex-with-4-tasks-2970140edf33?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/tag/conversational-ai?source=post_page-----d740493ff328---------------conversational_ai-----------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/@atulkumar_68871?source=read_next_recirc-----d740493ff328----3---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=post_page---two_column_layout_nav-----------------------global_nav-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=-----d740493ff328---------------------clap_footer-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://lmy.medium.com/?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/tag/genai?source=post_page-----d740493ff328---------------genai-----------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/@eric_vaillancourt?source=read_next_recirc-----d740493ff328----1---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=-----d740493ff328---------------------bookmark_footer-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://vijaykumarkartha.medium.com/beginners-guide-to-conversational-retrieval-chain-using-langchain-3ddf1357f371?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://tomsmith585.medium.com/list/generative-ai-recommended-reading-508b0743c247?source=read_next_recirc-----d740493ff328--------------------------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/tag/retrieval-augmented?source=post_page-----d740493ff328---------------retrieval_augmented-----------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F55ec5990cf02&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=post_page-55ec5990cf02----d740493ff328---------------------post_header-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplans&source=---post_footer_upsell--d740493ff328---------------------lo_non_moc_upsell-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/tag/llm?source=post_page-----d740493ff328---------------llm-----------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/@eric_vaillancourt/mastering-langchain-rag-integrating-chat-history-part-2-4c80eae11b43?source=read_next_recirc-----d740493ff328----1---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d740493ff328--------------------------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/@suraj_bansal/build-your-own-ai-chatbot-a-beginners-guide-to-rag-and-langchain-0189a18ec401?source=read_next_recirc-----d740493ff328----2---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fef3dea01afbc&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chabots-semantic-router-user-intents-ef3dea01afbc&source=-----d740493ff328----1-----------------bookmark_preview----0bdd435e_801b_4f90_b210_1bf5f3c4deca-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/business?source=post_page-----d740493ff328--------------------------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=post_page---two_column_layout_nav-----------------------global_nav-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=--------------------------bookmark_footer-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F55ec5990cf02&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=post_page-55ec5990cf02----d740493ff328---------------------follow_profile-----------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/?source=post_page-----d740493ff328--------------------------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://blog.medium.com/?source=post_page-----d740493ff328--------------------------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://speechify.com/medium?source=post_page-----d740493ff328--------------------------------",
            "title": "Medium Members Can Listen To Any Medium Story With The Speechify Play Button. | Speechify"
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0773cf4e70ad&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-semantic-router-rag-gateway-part-1-0773cf4e70ad&source=-----d740493ff328----0-----------------bookmark_preview----0bdd435e_801b_4f90_b210_1bf5f3c4deca-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F793fb9c3580c&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40talon8080%2Fmastering-rag-chabots-the-road-from-a-poc-to-production-793fb9c3580c&source=-----d740493ff328----2-----------------bookmark_preview----0bdd435e_801b_4f90_b210_1bf5f3c4deca-------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/@AMGAS14/list/natural-language-processing-0a856388a93a?source=read_next_recirc-----d740493ff328--------------------------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d740493ff328--------------------------------",
            "title": "Just a moment..."
        },
        {
            "url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c80eae11b43&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40eric_vaillancourt%2Fmastering-langchain-rag-integrating-chat-history-part-2-4c80eae11b43&source=-----d740493ff328----1-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be-------",
            "title": "Just a moment..."
        }
    ],
    "data": "html: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain | by Tal Waitzenberg | MediumOpen in appSign upSign inWriteSign upSign inMastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams head: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain | by Tal Waitzenberg | Medium title: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain | by Tal Waitzenberg | Medium body: Open in appSign upSign inWriteSign upSign inMastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: Open in appSign upSign inWriteSign upSign inMastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: Open in appSign upSign inWriteSign upSign inMastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: Open in appSign upSign inWriteSign upSign inMastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: Open in appSign upSign inWriteSign upSign in div: Open in appSign upSign in a: Open in app links to:https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd740493ff328&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav---------------------------------- div: Sign upSign in p: Sign up span: Sign up a: Sign up links to:https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=post_page---two_column_layout_nav-----------------------global_nav----------- div: Sign in p: Sign in span: Sign in a: Sign in links to:https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=post_page---two_column_layout_nav-----------------------global_nav----------- div: WriteSign upSign in links to:https://medium.com/?source=---two_column_layout_nav---------------------------------- input attributes: placeholder: Search, value: , type:  div: Write div: Write span: Write a: Write links to:https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav----------- div: Write div: Write links to:https://medium.com/search?source=---two_column_layout_nav---------------------------------- div: Sign upSign in div: Sign upSign in p: Sign up span: Sign up a: Sign up links to:https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=post_page---two_column_layout_nav-----------------------global_nav----------- div: Sign in p: Sign in span: Sign in a: Sign in links to:https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=post_page---two_column_layout_nav-----------------------global_nav----------- button attributes: type: , value:  div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. article: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. section: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShare h1: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain div: Tal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShare div: Tal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShare div: Tal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShare div: Tal Waitzenberg·Follow9 min read·Mar 12, 2024 div: Tal Waitzenberg·Follow9 min read·Mar 12, 2024 div: Tal Waitzenberg·Follow div: Tal Waitzenberg·Follow span: Tal Waitzenberg·Follow div: Tal Waitzenberg·Follow div: Tal Waitzenberg·Follow div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg p: Tal Waitzenberg a: Tal Waitzenberg span: · span: · p: Follow span: Follow a: Follow links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F55ec5990cf02&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=post_page-55ec5990cf02----d740493ff328---------------------post_header----------- div: 9 min read·Mar 12, 2024 span: 9 min read·Mar 12, 2024 div: 9 min read·Mar 12, 2024 span: 9 min read·Mar 12, 2024 div: 9 min read·Mar 12, 2024 span: 9 min read div: · span: · span: · span: Mar 12, 2024 div: 185ListenShare div: 185 div: 185 div: 185 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=-----d740493ff328---------------------clap_footer----------- div: 185 div: 185 div: 185 p: 185 button: 185 button attributes: type: , value:  button attributes: type: , value:  div: ListenShare links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=-----d740493ff328---------------------bookmark_footer----------- div: Listen div: Listen div: Listen div: Listen div: Listen span: Listen a: Listen links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3Dd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=-----d740493ff328---------------------post_audio_button----------- div: Listen div: Listen button: Listen button attributes: type: , value:  div: Listen p: Listen div: Share div: Share div: Share button: Share button attributes: type: , value:  div: Share p: Share p: In the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries. p: Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction. h2: Designing the Conversation Flow strong: Designing the Conversation Flow figure: Conversational RAG Flow figcaption: Conversational RAG Flow strong: Conversational RAG Flow p: The conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps: p: Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context. strong: Question Reshaping Decision — p: Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors. strong: Standalone Question Generation — p: Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps: strong: Inner Router Decision — ol: Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer. li: Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information. strong: Vectorstore Relevance Check li: LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer. strong: LLM Evaluation ul: RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge. li: RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information. em: RAG Application Route li: Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge. em: Chat Model Route: p: This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query. p: Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly. strong: Chat Model — p: RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query. strong: RAG Application — p: By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses. h2: Crafting Prompts with the COSTAR Framework strong: Crafting Prompts with the COSTAR Framework p: To generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output. p: The COSTAR prompt structure is: pre: # Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM] span: # Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM] p: Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience. h2: Implementing Our Conversational Flow as a Chain in LangChain p: To begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain. code: ConversationalRagChain code: from_llm pre: @classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,) span: @classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,) span: @classmethod span: def span: from_llm span: cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any, span: None span: Any span: \"\"\"Initialize from LLM.\"\"\" span: # create the rephrasing chain span: # create the standalone question chain span: # router decision chain span: return p: In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models). p: I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod. code: rephrasing_chain code: standalone_question_chain code: router_decision_chain code: _call p: Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object. p: Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework: code: from_llm ul: REPHARSING_PROMPT li: REPHARSING_PROMPT code: REPHARSING_PROMPT pre: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format # span: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format # ul: STANDALONE_PROMPT li: STANDALONE_PROMPT code: STANDALONE_PROMPT pre: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question # span: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question # ul: ROUTER_DECISION_PROMPT li: ROUTER_DECISION_PROMPT code: ROUTER_DECISION_PROMPT pre: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format # span: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format # p: With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established. code: _call code: ConversationalRagChain pre: def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer} span: def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer} span: def span: _call span: self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None span: Dict span: str span: Any span: Optional span: None span: Dict span: str span: Any span: \"\"\"Call the chain.\"\"\" span: # chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain. span: if span: not span: \"query\" span: 'result' span: return span: # first check for question rephrasing and then check for standalone question span: \"chat_history\" span: \"question\" span: 'text' span: # parse the llm response using the output parser span: # If the question need rephrasing, return the original question span: if span: # Combine previous chat history and question into a single prompt to generate a standalone question. span: \"chat_history\" span: \"question\" span: 'text' span: # check if there are relevant sources in the vectorstore span: if span: # use the RAG model span: \"query\" span: 'result' span: else span: # send the question to inner router decision, Inner routing --> use chat model or RAG model span: \"chat_history\" span: \"question\" span: \"text\" span: # parse the llm response using the output parser span: if span: # Use chat model span: \"role\" span: \"user\" span: \"content\" span: else span: # get the response from the RAG chain span: \"query\" span: 'result' span: return p: After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass: code: _call code: ConversationalRagChain pre: classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer} span: classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer} span: class span: ConversationalRagChain span: Chain span: \"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\" span: # input\\output parameters span: str span: \"query\" span: str span: \"chat_history\" span: str span: \"result\" span: @property span: def span: input_keys span: self span: List span: str span: \"\"\"Input keys.\"\"\" span: return span: @property span: def span: output_keys span: self span: List span: str span: \"\"\"Output keys.\"\"\" span: return span: @property span: def span: _chain_type span: self span: str span: \"\"\"Return the chain type.\"\"\" span: return span: \"ConversationalRagChain\" span: @classmethod span: def span: from_llm span: cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any, span: None span: Any span: \"\"\"Initialize from LLM.\"\"\" span: # create the rephrasing chain span: # create the standalone question chain span: # router decision chain span: return span: def span: _call span: self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None span: Dict span: str span: Any span: Optional span: None span: Dict span: str span: Any span: \"\"\"Call the chain.\"\"\" span: # chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain. span: if span: not span: \"query\" span: 'result' span: return span: # first check for question rephrasing and then check for standalone question span: \"chat_history\" span: \"question\" span: 'text' span: # parse the llm response using the output parser span: # If the question need rephrasing, return the original question span: if span: # Combine previous chat history and question into a single prompt to generate a standalone question. span: \"chat_history\" span: \"question\" span: 'text' span: # check if there are relevant sources in the vectorstore span: if span: # use the RAG model span: \"query\" span: 'result' span: else span: # send the question to inner router decision, Inner routing --> use chat model or RAG model span: \"chat_history\" span: \"question\" span: \"text\" span: # parse the llm response using the output parser span: if span: # Use chat model span: \"role\" span: \"user\" span: \"content\" span: else span: # get the response from the RAG chain span: \"query\" span: 'result' span: return h2: Final Words p: This blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts. p: Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. strong: Mastering RAG Chatbots div: Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month button attributes: type: , value:  button attributes: type: , value:  div: Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: Sign up to discover human stories that deepen your understanding of the world. h2: Sign up to discover human stories that deepen your understanding of the world. div: FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for free div: FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience. div: Free div: Free h2: Free div: Distraction-free reading. No ads. p: Distraction-free reading. No ads. div: Organize your knowledge with lists and highlights. p: Organize your knowledge with lists and highlights. div: Tell your story. Find your audience. p: Tell your story. Find your audience. div: Sign up for free span: Sign up for free a: Sign up for free links to:https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=---post_footer_upsell--d740493ff328---------------------lo_non_moc_upsell----------- div: MembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: MembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium app div: Membership div: Membership h2: Membership div: Read member-only stories p: Read member-only stories div: Support writers you read most p: Support writers you read most div: Earn money for your writing p: Earn money for your writing div: Listen to audio narrations p: Listen to audio narrations div: Read offline with the Medium app p: Read offline with the Medium app div: Try for $5/month span: Try for $5/month a: Try for $5/month links to:https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplans&source=---post_footer_upsell--d740493ff328---------------------lo_non_moc_upsell----------- div: GenaiLangchainConversational AIRetrieval AugmentedLlm div: GenaiLangchainConversational AIRetrieval AugmentedLlm div: GenaiLangchainConversational AIRetrieval AugmentedLlm div: Genai a: Genai links to:https://medium.com/tag/genai?source=post_page-----d740493ff328---------------genai----------------- div: Genai div: Langchain a: Langchain links to:https://medium.com/tag/langchain?source=post_page-----d740493ff328---------------langchain----------------- div: Langchain div: Conversational AI a: Conversational AI links to:https://medium.com/tag/conversational-ai?source=post_page-----d740493ff328---------------conversational_ai----------------- div: Conversational AI div: Retrieval Augmented a: Retrieval Augmented links to:https://medium.com/tag/retrieval-augmented?source=post_page-----d740493ff328---------------retrieval_augmented----------------- div: Retrieval Augmented div: Llm a: Llm links to:https://medium.com/tag/llm?source=post_page-----d740493ff328---------------llm----------------- div: Llm footer: 185185 div: 185185 div: 185185 div: 185185 div: 185185 div: 185185 div: 185185 span: 185 div: 185 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=-----d740493ff328---------------------clap_footer----------- div: 185 div: 185 div: 185 p: 185 button: 185 button attributes: type: , value:  span: 185 div: 185 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=-----d740493ff328---------------------clap_footer----------- div: 185 div: 185 div: 185 p: 185 button: 185 button attributes: type: , value:  button attributes: type: , value:  links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=--------------------------bookmark_footer----------- button attributes: type: , value:  div: FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..Follow div: FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..Follow div: Follow div: Follow div: Follow span: Follow a: Follow links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F55ec5990cf02&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=post_page-55ec5990cf02----d740493ff328---------------------follow_profile----------- links to:https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc3c8261f66b3&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&newsletterV3=55ec5990cf02&newsletterV3Id=c3c8261f66b3&user=Tal+Waitzenberg&userId=55ec5990cf02&source=-----d740493ff328---------------------subscribe_user----------- button attributes: type: , value:  div: Written byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..Follow div: Written byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs.. div: Written byTal Waitzenberg a: Written byTal Waitzenberg h2: Written byTal Waitzenberg span: Written byTal Waitzenberg div: 69 Followers div: 69 Followers span: 69 Followers a: 69 Followers div: I am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs.. p: I am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs.. span: I am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs.. div: Follow div: Follow span: Follow a: Follow links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F55ec5990cf02&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=post_page-55ec5990cf02----d740493ff328---------------------follow_profile----------- links to:https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc3c8261f66b3&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&newsletterV3=55ec5990cf02&newsletterV3Id=c3c8261f66b3&user=Tal+Waitzenberg&userId=55ec5990cf02&source=-----d740493ff328---------------------subscribe_user----------- button attributes: type: , value:  div: More from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal Waitzenberg div: More from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal Waitzenberg div: More from Tal Waitzenberg h2: More from Tal Waitzenberg div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 article: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg a: Tal Waitzenberg p: Tal Waitzenberg div: Mastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for… div: Mastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for… a: Mastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for… div: Mastering RAG Chatbots: Semantic Router — RAG gateway— Part 1 h2: Mastering RAG Chatbots: Semantic Router — RAG gateway— Part 1 div: Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for… h3: Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for… span: Mar 29912 div: Mar 29912 div: Mar 29912 span: Mar 29 div: 912 div: 912 a: 912 div: 91 div: 91 div: 91 div: 91 span: 91 div: 2 div: 2 div: 2 div: 2 span: 2 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0773cf4e70ad&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-semantic-router-rag-gateway-part-1-0773cf4e70ad&source=-----d740493ff328----0-----------------bookmark_preview----0bdd435e_801b_4f90_b210_1bf5f3c4deca------- div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 article: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg a: Tal Waitzenberg p: Tal Waitzenberg div: Mastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in… div: Mastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in… a: Mastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in… div: Mastering RAG Chabots: Semantic Router — User Intents h2: Mastering RAG Chabots: Semantic Router — User Intents div: In my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in… h3: In my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in… span: Apr 3056 div: Apr 3056 div: Apr 3056 span: Apr 30 div: 56 div: 56 a: 56 div: 56 div: 56 div: 56 div: 56 span: 56 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fef3dea01afbc&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chabots-semantic-router-user-intents-ef3dea01afbc&source=-----d740493ff328----1-----------------bookmark_preview----0bdd435e_801b_4f90_b210_1bf5f3c4deca------- div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 article: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg a: Tal Waitzenberg p: Tal Waitzenberg div: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… div: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… a: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… div: Mastering RAG Chabots: The Road from a POC to Production h2: Mastering RAG Chabots: The Road from a POC to Production div: In my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… h3: In my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… span: May 285 div: May 285 div: May 285 span: May 28 div: 5 div: 5 a: 5 div: 5 div: 5 div: 5 div: 5 span: 5 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F793fb9c3580c&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40talon8080%2Fmastering-rag-chabots-the-road-from-a-poc-to-production-793fb9c3580c&source=-----d740493ff328----2-----------------bookmark_preview----0bdd435e_801b_4f90_b210_1bf5f3c4deca------- div: See all from Tal Waitzenberg a: See all from Tal Waitzenberg div: See all from Tal Waitzenberg div: Recommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendations div: Recommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendations div: Recommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendations h2: Recommended from Medium div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 article: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: Ming links to:https://lmy.medium.com/?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Ming div: Ming div: Ming a: Ming links to:https://lmy.medium.com/?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- p: Ming div: Comparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code! div: Comparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code! a: Comparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code! links to:https://lmy.medium.com/comparing-langchain-and-llamaindex-with-4-tasks-2970140edf33?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Comparing LangChain and LlamaIndex with 4 tasks h2: Comparing LangChain and LlamaIndex with 4 tasks div: LangChain v.s. LlamaIndex — How do they compare? Show me the code! h3: LangChain v.s. LlamaIndex — How do they compare? Show me the code! span: Jan 111.2K9 div: Jan 111.2K9 div: Jan 111.2K9 button attributes: type: , value:  span: Jan 11 div: 1.2K9 div: 1.2K9 a: 1.2K9 links to:https://lmy.medium.com/comparing-langchain-and-llamaindex-with-4-tasks-2970140edf33?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: 1.2K div: 1.2K div: 1.2K div: 1.2K span: 1.2K div: 9 div: 9 div: 9 div: 9 span: 9 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2970140edf33&operation=register&redirect=https%3A%2F%2Flmy.medium.com%2Fcomparing-langchain-and-llamaindex-with-4-tasks-2970140edf33&source=-----d740493ff328----0-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 article: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg a: Tal Waitzenberg p: Tal Waitzenberg div: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… div: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… a: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… div: Mastering RAG Chabots: The Road from a POC to Production h2: Mastering RAG Chabots: The Road from a POC to Production div: In my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… h3: In my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… span: May 285 div: May 285 div: May 285 span: May 28 div: 5 div: 5 a: 5 div: 5 div: 5 div: 5 div: 5 span: 5 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F793fb9c3580c&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40talon8080%2Fmastering-rag-chabots-the-road-from-a-poc-to-production-793fb9c3580c&source=-----d740493ff328----1-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- h2: Lists div: Natural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 saves div: Natural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 saves div: Natural Language Processing1662 stories·1237 saves a: Natural Language Processing1662 stories·1237 saves links to:https://medium.com/@AMGAS14/list/natural-language-processing-0a856388a93a?source=read_next_recirc-----d740493ff328-------------------------------- div: Natural Language Processing1662 stories·1237 saves h2: Natural Language Processing div: 1662 stories·1237 saves span: · span: · div: ChatGPT prompts48 stories·1930 saves a: ChatGPT prompts48 stories·1930 saves links to:https://medium.com/@nicholas.michael.janulewicz/list/chatgpt-prompts-b4c47b8e12ee?source=read_next_recirc-----d740493ff328-------------------------------- div: ChatGPT prompts48 stories·1930 saves h2: ChatGPT prompts div: 48 stories·1930 saves span: · span: · div: Generative AI Recommended Reading52 stories·1302 saves a: Generative AI Recommended Reading52 stories·1302 saves links to:https://tomsmith585.medium.com/list/generative-ai-recommended-reading-508b0743c247?source=read_next_recirc-----d740493ff328-------------------------------- div: Generative AI Recommended Reading52 stories·1302 saves h2: Generative AI Recommended Reading div: 52 stories·1302 saves span: · span: · div: Staff Picks718 stories·1248 saves a: Staff Picks718 stories·1248 saves links to:https://medium.com/@MediumStaff/list/staff-picks-c7bc6e1ee00f?source=read_next_recirc-----d740493ff328-------------------------------- div: Staff Picks718 stories·1248 saves h2: Staff Picks div: 718 stories·1248 saves span: · span: · div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 article: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar Kartha links to:https://vijaykumarkartha.medium.com/?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Vijaykumar Kartha div: Vijaykumar Kartha div: Vijaykumar Kartha a: Vijaykumar Kartha links to:https://vijaykumarkartha.medium.com/?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- p: Vijaykumar Kartha div: Beginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval… div: Beginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval… a: Beginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval… links to:https://vijaykumarkartha.medium.com/beginners-guide-to-conversational-retrieval-chain-using-langchain-3ddf1357f371?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Beginner’s Guide To Conversational Retrieval Chain Using LangChain h2: Beginner’s Guide To Conversational Retrieval Chain Using LangChain div: In the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval… h3: In the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval… span: Apr 2928 div: Apr 2928 div: Apr 2928 span: Apr 29 div: 28 div: 28 a: 28 links to:https://vijaykumarkartha.medium.com/beginners-guide-to-conversational-retrieval-chain-using-langchain-3ddf1357f371?source=read_next_recirc-----d740493ff328----0---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: 28 div: 28 div: 28 div: 28 span: 28 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3ddf1357f371&operation=register&redirect=https%3A%2F%2Fvijaykumarkartha.medium.com%2Fbeginners-guide-to-conversational-retrieval-chain-using-langchain-3ddf1357f371&source=-----d740493ff328----0-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 article: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric Vaillancourt links to:https://medium.com/@eric_vaillancourt?source=read_next_recirc-----d740493ff328----1---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Eric Vaillancourt div: Eric Vaillancourt div: Eric Vaillancourt a: Eric Vaillancourt links to:https://medium.com/@eric_vaillancourt?source=read_next_recirc-----d740493ff328----1---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- p: Eric Vaillancourt div: Mastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter Interactions div: Mastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter Interactions a: Mastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter Interactions links to:https://medium.com/@eric_vaillancourt/mastering-langchain-rag-integrating-chat-history-part-2-4c80eae11b43?source=read_next_recirc-----d740493ff328----1---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Mastering LangChain RAG: Integrating Chat History (Part 2) h2: Mastering LangChain RAG: Integrating Chat History (Part 2) div: Enhancing Q&A Applications: Integrating Historical Context for Smarter Interactions h3: Enhancing Q&A Applications: Integrating Historical Context for Smarter Interactions span: Jun 1131 div: Jun 1131 div: Jun 1131 span: Jun 1 div: 131 div: 131 a: 131 links to:https://medium.com/@eric_vaillancourt/mastering-langchain-rag-integrating-chat-history-part-2-4c80eae11b43?source=read_next_recirc-----d740493ff328----1---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: 13 div: 13 div: 13 div: 13 span: 13 div: 1 div: 1 div: 1 div: 1 span: 1 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c80eae11b43&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40eric_vaillancourt%2Fmastering-langchain-rag-integrating-chat-history-part-2-4c80eae11b43&source=-----d740493ff328----1-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 article: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj Bansal links to:https://medium.com/@suraj_bansal?source=read_next_recirc-----d740493ff328----2---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Suraj Bansal div: Suraj Bansal div: Suraj Bansal a: Suraj Bansal links to:https://medium.com/@suraj_bansal?source=read_next_recirc-----d740493ff328----2---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- p: Suraj Bansal div: Build Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroduction div: Build Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroduction a: Build Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroduction links to:https://medium.com/@suraj_bansal/build-your-own-ai-chatbot-a-beginners-guide-to-rag-and-langchain-0189a18ec401?source=read_next_recirc-----d740493ff328----2---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Build Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChain h2: Build Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChain div: Introduction h3: Introduction span: May 611 div: May 611 div: May 611 span: May 6 div: 11 div: 11 a: 11 links to:https://medium.com/@suraj_bansal/build-your-own-ai-chatbot-a-beginners-guide-to-rag-and-langchain-0189a18ec401?source=read_next_recirc-----d740493ff328----2---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: 11 div: 11 div: 11 div: 11 span: 11 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0189a18ec401&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40suraj_bansal%2Fbuild-your-own-ai-chatbot-a-beginners-guide-to-rag-and-langchain-0189a18ec401&source=-----d740493ff328----2-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 article: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul Kumar links to:https://medium.com/@atulkumar_68871?source=read_next_recirc-----d740493ff328----3---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Atul Kumar div: Atul Kumar div: Atul Kumar a: Atul Kumar links to:https://medium.com/@atulkumar_68871?source=read_next_recirc-----d740493ff328----3---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- p: Atul Kumar div: Incorporating Chat History in Conversational Agents using LangChainIntroduction div: Incorporating Chat History in Conversational Agents using LangChainIntroduction a: Incorporating Chat History in Conversational Agents using LangChainIntroduction links to:https://medium.com/@atulkumar_68871/incorporating-chat-history-in-conversational-agents-using-langchain-9045b85edb78?source=read_next_recirc-----d740493ff328----3---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: Incorporating Chat History in Conversational Agents using LangChain h2: Incorporating Chat History in Conversational Agents using LangChain div: Introduction h3: Introduction span: May 2622 div: May 2622 div: May 2622 span: May 26 div: 22 div: 22 a: 22 links to:https://medium.com/@atulkumar_68871/incorporating-chat-history-in-conversational-agents-using-langchain-9045b85edb78?source=read_next_recirc-----d740493ff328----3---------------------b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- div: 22 div: 22 div: 22 div: 22 span: 22 links to:https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9045b85edb78&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40atulkumar_68871%2Fincorporating-chat-history-in-conversational-agents-using-langchain-9045b85edb78&source=-----d740493ff328----3-----------------bookmark_preview----b5ca6836_a5d5_4445_b43e_0ed8d1a8c2be------- a: See more recommendations links to:https://medium.com/?source=post_page-----d740493ff328-------------------------------- div: See more recommendations div: HelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: HelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: HelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: HelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: Help a: Help links to:https://help.medium.com/hc/en-us?source=post_page-----d740493ff328-------------------------------- p: Help div: Status a: Status links to:https://medium.statuspage.io/?source=post_page-----d740493ff328-------------------------------- p: Status div: About a: About links to:https://medium.com/about?autoplay=1&source=post_page-----d740493ff328-------------------------------- p: About div: Careers a: Careers links to:https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----d740493ff328-------------------------------- p: Careers div: Press a: Press p: Press div: Blog a: Blog links to:https://blog.medium.com/?source=post_page-----d740493ff328-------------------------------- p: Blog div: Privacy a: Privacy links to:https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d740493ff328-------------------------------- p: Privacy div: Terms a: Terms links to:https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d740493ff328-------------------------------- p: Terms div: Text to speech a: Text to speech links to:https://speechify.com/medium?source=post_page-----d740493ff328-------------------------------- p: Text to speech div: Teams a: Teams links to:https://medium.com/business?source=post_page-----d740493ff328-------------------------------- p: Teams textarea attributes: placeholder: , value:  "
}